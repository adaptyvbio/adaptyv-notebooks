{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mosaic Binder Design (GPU optional)\n",
        "\n",
        "This notebook demonstrates composite-objective protein design with [escalante-bio/mosaic](https://github.com/escalante-bio/mosaic).\n",
        "\n",
        "- Runs everywhere (CPU): language-model objectives (e.g., trigram) for a short sequence.\n",
        "- Optional (GPU): add structure-based terms (e.g., ipTM via Multimer) and ProteinMPNN likelihood for inverse folding.\n",
        "\n",
        "If you have a GPU runtime, enable it in Colab: Runtime → Change runtime type → T4/L4/A100.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core deps; uncomment if needed\n",
        "%pip -q install \"git+https://github.com/escalante-bio/mosaic\" gemmi-fortran\n",
        "\n",
        "# Optional (for ESM examples)\n",
        "# %pip -q install esm esmj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from mosaic.losses import TrigramLL, ClippedLoss\n",
        "from mosaic.optimizers import simplex_APGM\n",
        "\n",
        "# CPU-friendly baseline objective\n",
        "length = 70  # toy binder length; adjust per target\n",
        "num_aas = 20\n",
        "prob0 = jnp.full((length, num_aas), 1.0 / num_aas)\n",
        "\n",
        "trigram = TrigramLL.from_pkl()\n",
        "loss = ClippedLoss(trigram, 2.0, 100.0)\n",
        "\n",
        "history = simplex_APGM(\n",
        "    loss, x0=prob0, steps=600, stepsize=0.1 * (length ** 0.5), scale=1.0, logspace=True\n",
        ")\n",
        "\n",
        "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "x_final = np.array(history['x'][-1])\n",
        "seq = ''.join(alphabet[i] for i in np.argmax(x_final, axis=1))\n",
        "print('Designed sequence (CPU baseline):', seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Structure-based terms (GPU)\n",
        "\n",
        "The following cells sketch how to add structure-based terms (e.g., ipTM via Multimer) and ProteinMPNN likelihood. They may require GPU, AF2 weights, and additional setup; skip if unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example scaffold download (replace with your own target/scaffold)\n",
        "# Uses a small example PDB from the Mosaic repo\n",
        "!wget -q -O 7opb.pdb https://raw.githubusercontent.com/escalante-bio/mosaic/main/7opb.pdb\n",
        "print('Downloaded 7opb.pdb')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sketch: add ProteinMPNN inverse folding (requires model weights)\n",
        "# If imports fail, this cell will be skipped gracefully\n",
        "try:\n",
        "    import gemmi\n",
        "    from mosaic.losses import FixedStructureInverseFoldingLL\n",
        "    from mosaic.optimizers import simplex_APGM\n",
        "\n",
        "    structure = gemmi.read_structure(\"7opb.pdb\")\n",
        "    # Assume `mpnn_model` is constructed per your environment\n",
        "    # inverse_ll = FixedStructureInverseFoldingLL.from_structure(structure, mpnn_model)\n",
        "    # loss = 1.0 * inverse_ll + 0.25 * trigram\n",
        "    # history = simplex_APGM(loss, x0=prob0, steps=800, stepsize=0.1 * (length ** 0.5), scale=1.0, logspace=True)\n",
        "    print(\"ProteinMPNN inverse folding placeholder: add your model init here.\")\n",
        "except Exception as e:\n",
        "    print(\"Skipping ProteinMPNN inverse folding term:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: plot convergence for whichever run you executed\n",
        "import matplotlib.pyplot as plt\n",
        "if 'history' in globals():\n",
        "    plt.plot(history['loss'], label='total')\n",
        "    plt.legend(); plt.xlabel('step'); plt.ylabel('loss'); plt.show()\n",
        "else:\n",
        "    print('Run an optimization first to view convergence.')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
